import streamlit as st
import torch
import tempfile
import os
import gc
from datetime import datetime

# LangChain and HuggingFace imports
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_huggingface.llms import HuggingFacePipeline
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Transformers imports
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig

# Enhanced chunking imports
from enhanced_chunking import (
    create_enhanced_chunker, 
    get_chunking_stats, 
    ChunkingConfig,
    EnhancedChunker,
    ChunkingCache
)

# =================================================================
# 1. C·∫§U H√åNH TRANG V√Ä SESSION STATE
# =================================================================

st.set_page_config(
    page_title="ü§ñ AI RAG Assistant - Enhanced", 
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Kh·ªüi t·∫°o session state
for key in ["rag_chain", "models_loaded", "embeddings", "llm", "messages", "chat_history", "current_chat_id", "chunking_settings"]:
    if key not in st.session_state:
        if key == "messages":
            st.session_state[key] = []
        elif key == "chat_history":
            st.session_state[key] = []
        elif key == "current_chat_id":
            st.session_state[key] = None  # None = chat m·ªõi ch∆∞a l∆∞u
        elif key == "chunking_settings":
            st.session_state[key] = {
                "strategy": "hybrid",
                "chunk_size": 1000,
                "overlap": 100,
                "enable_cache": True,
                "show_progress": True
            }
        else:
            st.session_state[key] = None

# =================================================================
# 2. C√ÅC H√ÄM T·∫¢I MODEL (T·ªêI ·ªÆU CHO COLAB GPU T4)
# =================================================================

@st.cache_resource
def load_embeddings():
    """T·∫£i model embedding ƒë∆∞·ª£c t·ªëi ∆∞u cho ti·∫øng Vi·ªát."""
    model_name = "bkai-foundation-models/vietnamese-bi-encoder"
    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    return HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

@st.cache_resource
def load_llm():
    """T·∫£i Large Language Model (Vicuna 7B) v·ªõi quantization ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n."""
    # C·∫•u h√¨nh quantization t·ªëi ∆∞u cho T4
    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    MODEL_NAME = "lmsys/vicuna-7b-v1.5"

    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=nf4_config,
            low_cpu_mem_usage=True,
            device_map="auto",
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        
        # ƒê·∫£m b·∫£o c√≥ pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=1024,
            pad_token_id=tokenizer.eos_token_id,
            device_map="auto",
            do_sample=True,
            temperature=0.7,
            top_p=0.95
        )

        # D·ªçn d·∫πp b·ªô nh·ªõ
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return HuggingFacePipeline(pipeline=model_pipeline)
    
    except Exception as e:
        return None

# =================================================================
# 3. H√ÄM X·ª¨ L√ù PDF V√Ä T·∫†O RAG CHAIN (ENHANCED VERSION)
# =================================================================

def process_pdf(uploaded_file):
    """X·ª≠ l√Ω file PDF ƒë∆∞·ª£c t·∫£i l√™n v√† t·∫°o RAG chain v·ªõi Enhanced Chunking."""
    
    try:
        # T·∫°o file t·∫°m
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # Load t√†i li·ªáu
        st.info("üìÑ ƒêang ƒë·ªçc file PDF...")
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        
        # L·∫•y settings t·ª´ session state
        settings = st.session_state.get("chunking_settings", {
            "strategy": "hybrid",
            "chunk_size": 1000,
            "overlap": 100,
            "enable_cache": True,
            "show_progress": True
        })

        # T·∫°o custom config cho enhanced chunker
        config = ChunkingConfig()
        config.strategy = settings["strategy"]
        config.fixed_chunk_size = settings["chunk_size"]
        config.fixed_overlap = settings["overlap"]
        config.enable_cache = settings["enable_cache"]
        config.show_progress = settings["show_progress"]

        # T·∫°o enhanced chunker
        enhanced_chunker = EnhancedChunker(st.session_state.embeddings, config)

        # Chunking v·ªõi metadata
        st.info("‚úÇÔ∏è ƒêang chia nh·ªè t√†i li·ªáu v·ªõi Enhanced Chunking...")
        docs, chunking_metadata = enhanced_chunker.chunk_documents(documents)

        # Hi·ªÉn th·ªã th·ªëng k√™ chunking
        with st.expander("üìä Chunking Statistics", expanded=True):
            st.markdown(get_chunking_stats(chunking_metadata))
            
            # Th√™m th√¥ng tin chi ti·∫øt
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üì¶ Total Chunks", chunking_metadata.get('num_chunks', 0))
            with col2:
                st.metric("‚è±Ô∏è Processing Time", f"{chunking_metadata.get('processing_time', 0):.1f}s")
            with col3:
                cache_status = "‚úÖ Hit" if chunking_metadata.get('cache_hit') else "‚ùå Miss"
                st.metric("üíæ Cache", cache_status)

        st.info("üîç ƒêang t·∫°o c∆° s·ªü d·ªØ li·ªáu vector...")
        
        # T·∫°o Vector Database
        vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
        retriever = vector_db.as_retriever(search_kwargs={"k": 5})

        # T·∫°o prompt template t·ªëi ∆∞u cho ti·∫øng Vi·ªát
        RAG_PROMPT_TEMPLATE = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† ch√≠nh x√°c
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng c√≥ ƒë·ªß th√¥ng tin

Tr·∫£ l·ªùi:"""

        prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | st.session_state.llm
            | StrOutputParser()
            | (lambda text: text.split("Tr·∫£ l·ªùi:")[-1].strip() if "Tr·∫£ l·ªùi:" in text else text.strip())
        )

        # D·ªçn d·∫πp
        os.unlink(tmp_file_path)
        del loader, documents, vector_db
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return rag_chain, len(docs), chunking_metadata
        
    except Exception as e:
        st.error(f"‚ùå L·ªói x·ª≠ l√Ω PDF: {str(e)}")
        return None, 0, None

# =================================================================
# 4. H√ÄM QU·∫¢N L√ù L·ªäCH S·ª¨ CHAT (UNCHANGED)
# =================================================================

def auto_save_current_chat():
    """T·ª± ƒë·ªông l∆∞u chat hi·ªán t·∫°i (ghi ƒë√® n·∫øu ƒë√£ c√≥, t·∫°o m·ªõi n·∫øu ch∆∞a)."""
    try:
        if st.session_state.messages and len(st.session_state.messages) > 0:
            # T·∫°o title d·ª±a tr√™n c√¢u h·ªèi ƒë·∫ßu ti√™n
            first_question = st.session_state.messages[0]["content"][:30] + "..." if len(st.session_state.messages[0]["content"]) > 30 else st.session_state.messages[0]["content"]
            
            chat_session = {
                "timestamp": datetime.now().strftime("%H:%M %d/%m"),
                "messages": st.session_state.messages.copy(),
                "title": first_question
            }
            
            if (st.session_state.current_chat_id is not None and 
                0 <= st.session_state.current_chat_id < len(st.session_state.chat_history)):
                # Ghi ƒë√® chat ƒë√£ c√≥
                st.session_state.chat_history[st.session_state.current_chat_id] = chat_session
            else:
                # T·∫°o chat m·ªõi
                st.session_state.chat_history.append(chat_session)
                st.session_state.current_chat_id = len(st.session_state.chat_history) - 1
            return True
    except Exception as e:
        # N·∫øu c√≥ l·ªói, v·∫´n t·∫°o chat m·ªõi
        try:
            if st.session_state.messages and len(st.session_state.messages) > 0:
                first_question = st.session_state.messages[0]["content"][:30] + "..."
                chat_session = {
                    "timestamp": datetime.now().strftime("%H:%M %d/%m"),
                    "messages": st.session_state.messages.copy(),
                    "title": first_question
                }
                st.session_state.chat_history.append(chat_session)
                st.session_state.current_chat_id = len(st.session_state.chat_history) - 1
                return True
        except:
            pass
    return False

def create_new_chat():
    """T·∫°o chat m·ªõi v√† l∆∞u chat hi·ªán t·∫°i."""
    # L∆∞u chat hi·ªán t·∫°i n·∫øu c√≥
    auto_save_current_chat()
    
    # T·∫°o chat m·ªõi
    st.session_state.messages = []
    st.session_state.current_chat_id = None  # Chat m·ªõi ch∆∞a l∆∞u
    return True

def load_chat_from_history(index):
    """T·∫£i cu·ªôc tr√≤ chuy·ªán t·ª´ l·ªãch s·ª≠."""
    try:
        if 0 <= index < len(st.session_state.chat_history):
            # L∆∞u chat hi·ªán t·∫°i tr∆∞·ªõc khi chuy·ªÉn
            auto_save_current_chat()
            
            # Load chat ƒë∆∞·ª£c ch·ªçn
            st.session_state.messages = st.session_state.chat_history[index]["messages"].copy()
            st.session_state.current_chat_id = index
            return True
    except (IndexError, KeyError):
        # N·∫øu c√≥ l·ªói, reset v·ªÅ chat m·ªõi
        st.session_state.messages = []
        st.session_state.current_chat_id = None
    return False

def delete_chat_from_history(index):
    """X√≥a m·ªôt chat kh·ªèi l·ªãch s·ª≠."""
    try:
        if 0 <= index < len(st.session_state.chat_history):
            # L∆∞u current_chat_id c≈© ƒë·ªÉ so s√°nh
            old_current_id = st.session_state.current_chat_id
            
            # X√≥a chat kh·ªèi l·ªãch s·ª≠ tr∆∞·ªõc
            st.session_state.chat_history.pop(index)
            
            # C·∫≠p nh·∫≠t current_chat_id sau khi x√≥a
            if old_current_id == index:
                # N·∫øu x√≥a chat ƒëang active, chuy·ªÉn v·ªÅ chat m·ªõi
                st.session_state.messages = []
                st.session_state.current_chat_id = None
            elif old_current_id is not None and old_current_id > index:
                # N·∫øu x√≥a chat c√≥ index nh·ªè h∆°n current_chat_id, gi·∫£m current_chat_id ƒëi 1
                st.session_state.current_chat_id = old_current_id - 1
            
            # ƒê·∫£m b·∫£o current_chat_id kh√¥ng v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng chat c√≤n l·∫°i
            if (st.session_state.current_chat_id is not None and 
                st.session_state.current_chat_id >= len(st.session_state.chat_history)):
                st.session_state.current_chat_id = None
                st.session_state.messages = []
            
            return True
    except Exception as e:
        # X·ª≠ l√Ω l·ªói v√† reset v·ªÅ tr·∫°ng th√°i an to√†n
        st.session_state.current_chat_id = None
        st.session_state.messages = []
        return False
    return False

def clear_chat_history():
    """X√≥a to√†n b·ªô l·ªãch s·ª≠ chat."""
    st.session_state.chat_history = []
    st.session_state.messages = []
    st.session_state.current_chat_id = None
    st.session_state['refresh_needed'] = True

# =================================================================
# 5. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (ENHANCED VERSION)
# =================================================================

# Header ch√≠nh
st.title("üöÄ ·ª®ng d·ª•ng RAG Enhanced - H·ªèi ƒë√°p t√†i li·ªáu PDF")
st.markdown("*Tr√≤ chuy·ªán th√¥ng minh v·ªõi t√†i li·ªáu c·ªßa b·∫°n b·∫±ng ti·∫øng Vi·ªát - V·ªõi Enhanced Chunking*")

# === SIDEBAR ===
with st.sidebar:
    st.header("üìÇ Qu·∫£n l√Ω t√†i li·ªáu")
    
    # Upload file
    uploaded_file = st.file_uploader(
        "Ch·ªçn file PDF", 
        type="pdf",
        help="H·ªó tr·ª£ file PDF c√≥ k√≠ch th∆∞·ªõc t·ªëi ƒëa 200MB"
    )
    
    # === ENHANCED CHUNKING SETTINGS ===
    st.markdown("---")
    st.subheader("‚öôÔ∏è Enhanced Chunking Settings")

    # Chunking strategy selection
    strategy = st.selectbox(
        "üìä Strategy",
        ["hybrid", "semantic", "fixed"],
        index=0,
        help="‚Ä¢ Hybrid: Balance t·ªëc ƒë·ªô vs ch·∫•t l∆∞·ª£ng\n‚Ä¢ Semantic: Ch·∫•t l∆∞·ª£ng cao (ch·∫≠m)\n‚Ä¢ Fixed: T·ªëc ƒë·ªô cao (nhanh)",
        key="chunking_strategy"
    )

    # Advanced settings trong expander
    with st.expander("üîß Advanced Settings"):
        chunk_size = st.slider("üìè Chunk Size", 500, 2000, 1000, 100, 
                              help="K√≠ch th∆∞·ªõc chunk (characters)")
        overlap = st.slider("üîó Overlap", 0, 300, 100, 50,
                          help="Overlap gi·ªØa chunks ƒë·ªÉ preserve context")
        enable_cache = st.checkbox("üíæ Enable Cache", True,
                                 help="Cache chunks ƒë·ªÉ tƒÉng t·ªëc l·∫ßn sau")
        show_progress = st.checkbox("üìä Show Progress", True,
                                  help="Hi·ªÉn th·ªã progress bars")
        
        # Clear cache button
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üóëÔ∏è Clear Cache", help="X√≥a to√†n b·ªô chunking cache"):
                try:
                    cache = ChunkingCache()
                    cache.clear_cache()
                    st.success("Cache cleared!")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        with col2:
            # Show cache info
            if st.button("‚ÑπÔ∏è Cache Info"):
                try:
                    cache = ChunkingCache()
                    cache_info = cache.get_cache_info()
                    st.info(f"üì¶ {cache_info['cache_files']} cached documents\nüíæ {cache_info['total_size_mb']:.1f} MB used")
                except Exception as e:
                    st.error(f"Error: {e}")

    # L∆∞u settings v√†o session state
    st.session_state.chunking_settings = {
        "strategy": strategy,
        "chunk_size": chunk_size,
        "overlap": overlap,
        "enable_cache": enable_cache,
        "show_progress": show_progress
    }

    # Process PDF button
    if uploaded_file and st.session_state.models_loaded:
        if st.button("üöÄ X·ª≠ l√Ω PDF v·ªõi Enhanced Chunking", type="primary"):
            if st.session_state.embeddings and st.session_state.llm:
                # L∆∞u chat hi·ªán t·∫°i tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu chat m·ªõi v·ªõi t√†i li·ªáu m·ªõi
                auto_save_current_chat()
                
                # Reset v·ªÅ chat m·ªõi
                st.session_state.messages = []
                st.session_state.current_chat_id = None
                
                rag_chain, num_chunks, chunking_metadata = process_pdf(uploaded_file)
                if rag_chain and num_chunks > 0:
                    st.session_state.rag_chain = rag_chain
                    
                    # Enhanced success message with stats
                    strategy_used = chunking_metadata.get('strategy', 'unknown')
                    processing_time = chunking_metadata.get('processing_time', 0)
                    cache_hit = chunking_metadata.get('cache_hit', False)
                    
                    success_msg = f"‚úÖ Th√†nh c√¥ng! {num_chunks} chunks ({strategy_used})"
                    if cache_hit:
                        success_msg += " - Cache Hit! ‚ö°"
                    else:
                        success_msg += f" trong {processing_time:.1f}s"
                    
                    st.success(success_msg)
                    st.info("üí¨ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán!")
                else:
                    st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file PDF. Vui l√≤ng th·ª≠ file kh√°c.")
            else:
                st.error("‚ùå Models ch∆∞a s·∫µn s√†ng!")
    elif uploaded_file and not st.session_state.models_loaded:
        st.warning("‚è≥ Vui l√≤ng ch·ªù h·ªá th·ªëng t·∫£i models xong.")

    st.markdown("---")
    
    # Qu·∫£n l√Ω chat - ch·ªâ hi·ªÉn th·ªã khi c√≥ RAG chain
    if st.session_state.rag_chain:
        st.header("üí¨ Qu·∫£n l√Ω cu·ªôc tr√≤ chuy·ªán")
        
        # Hi·ªÉn th·ªã tr·∫°ng th√°i chat hi·ªán t·∫°i
        try:
            if (st.session_state.current_chat_id is not None and 
                0 <= st.session_state.current_chat_id < len(st.session_state.chat_history)):
                current_title = st.session_state.chat_history[st.session_state.current_chat_id]["title"]
                st.info(f"üìù ƒêang ch·ªânh s·ª≠a: {current_title}", icon="‚úèÔ∏è")
            else:
                st.info("üÜï Chat m·ªõi (ch∆∞a l∆∞u)", icon="üí¨")
        except (IndexError, KeyError):
            # N·∫øu c√≥ l·ªói, reset v·ªÅ chat m·ªõi
            st.session_state.current_chat_id = None
            st.info("üÜï Chat m·ªõi (ch∆∞a l∆∞u)", icon="üí¨")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üÜï M·ªü chat m·ªõi", help="L∆∞u chat hi·ªán t·∫°i v√† t·∫°o chat m·ªõi", key="new_chat"):
                if create_new_chat():
                    st.session_state['refresh_needed'] = True
        
        with col2:
            if st.button("üóëÔ∏è X√≥a", help="X√≥a cu·ªôc tr√≤ chuy·ªán hi·ªán t·∫°i", key="clear_current"):
                st.session_state.messages = []
                st.session_state.current_chat_id = None
                st.session_state['refresh_needed'] = True
        
        # L·ªãch s·ª≠ chat
        if st.session_state.chat_history:
            st.subheader("üìö L·ªãch s·ª≠")
            
            # Hi·ªÉn th·ªã t·ªëi ƒëa 5 chat g·∫ßn nh·∫•t
            display_count = min(5, len(st.session_state.chat_history))
            start_index = len(st.session_state.chat_history) - display_count
            
            for i in range(display_count):
                chat_index = start_index + i
                
                # Ki·ªÉm tra bounds an to√†n
                if chat_index >= len(st.session_state.chat_history):
                    continue
                    
                try:
                    chat = st.session_state.chat_history[chat_index]
                    
                    # Highlight chat ƒëang active
                    is_current = (st.session_state.current_chat_id == chat_index)
                    button_type = "primary" if is_current else "secondary"
                    icon = "üìù" if is_current else "üìñ"
                    
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        # S·ª≠ d·ª•ng unique key cho m·ªói button
                        safe_timestamp = chat['timestamp'].replace('/', '_').replace(' ', '_').replace(':', '_')
                        button_key = f"load_chat_{chat_index}_{safe_timestamp}"
                        if st.button(f"{icon} {chat['title']}", key=button_key, help=f"{chat['timestamp']} {'(ƒêang ch·ªânh s·ª≠a)' if is_current else ''}", type=button_type):
                            if not is_current:  # Ch·ªâ load n·∫øu kh√¥ng ph·∫£i chat hi·ªán t·∫°i
                                load_chat_from_history(chat_index)
                                st.session_state['refresh_needed'] = True
                    
                    with col2:
                        delete_key = f"delete_chat_{chat_index}_{safe_timestamp}"
                        if st.button("üóëÔ∏è", key=delete_key, help="X√≥a chat n√†y"):
                            delete_chat_from_history(chat_index)
                            st.session_state['refresh_needed'] = True
                            
                except (IndexError, KeyError):
                    # B·ªè qua chat b·ªã l·ªói
                    continue
            
            if len(st.session_state.chat_history) > 5:
                st.caption(f"Hi·ªÉn th·ªã {display_count}/{len(st.session_state.chat_history)} chat g·∫ßn nh·∫•t")
            
            if st.button("üóëÔ∏è X√≥a t·∫•t c·∫£", type="secondary", key="clear_all_chats"):
                clear_chat_history()

# === MAIN CONTENT ===

# === MAIN CONTENT - T·∫¢I MODELS ===
if not st.session_state.models_loaded:
    st.info("üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng AI... Vui l√≤ng ch·ªù trong gi√¢y l√°t")
    
    # Hi·ªÉn th·ªã th√¥ng tin GPU
    if torch.cuda.is_available():
        st.success(f"‚úÖ Ph√°t hi·ªán GPU: {torch.cuda.get_device_name()}")
    else:
        st.warning("‚ö†Ô∏è Ch·ªâ s·ª≠ d·ª•ng CPU - c√≥ th·ªÉ ch·∫≠m h∆°n")
    
    # Container ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh
    progress_container = st.container()
    
    # T·∫£i embedding
    with progress_container:
        st.write("üì• ƒêang t·∫£i Embedding Model...")
        try:
            if not st.session_state.embeddings:
                st.session_state.embeddings = load_embeddings()
            st.write("‚úÖ Embedding Model ƒë√£ s·∫µn s√†ng")
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫£i Embedding: {str(e)}")
            st.stop()
    
    # T·∫£i LLM
    with progress_container:
        st.write("ü§ñ ƒêang t·∫£i Large Language Model...")
        try:
            if not st.session_state.llm:
                st.session_state.llm = load_llm()
            
            if st.session_state.llm:
                st.write("‚úÖ LLM ƒë√£ s·∫µn s√†ng")
                st.session_state.models_loaded = True
                progress_container.success("üéâ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng! H√£y t·∫£i file PDF ·ªü sidebar.")
                st.balloons()
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i LLM")
                st.stop()
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫£i LLM: {str(e)}")
            st.stop()

# === MAIN CONTENT - CHAT INTERFACE ===

# Giao di·ªán chat ch√≠nh
if st.session_state.rag_chain:
    st.header("üí¨ Tr√≤ chuy·ªán v·ªõi t√†i li·ªáu")
    
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input chat
    if prompt := st.chat_input("üí≠ H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu..."):
        # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # T·∫°o v√† hi·ªÉn th·ªã ph·∫£n h·ªìi
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            response_placeholder.info("ü§î ƒêang suy nghƒ©...")
            
            try:
                response = st.session_state.rag_chain.invoke(prompt)
                response_placeholder.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                # T·ª± ƒë·ªông l∆∞u chat sau khi c√≥ ph·∫£n h·ªìi
                auto_save_current_chat()
                
            except Exception as e:
                error_msg = f"‚ùå C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i."
                response_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

elif st.session_state.models_loaded:
    # H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng khi models ƒë√£ t·∫£i
    st.info("üöÄ **H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!** H√£y t·∫£i file PDF ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    
    # Th√¥ng tin h·ªá th·ªëng
    with st.expander("üîß Th√¥ng tin h·ªá th·ªëng"):
        col1, col2 = st.columns(2)
        with col1:
            st.write("**GPU:**", torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU")
            st.write("**Models:**", "‚úÖ ƒê√£ t·∫£i" if st.session_state.models_loaded else "‚ùå Ch∆∞a t·∫£i")
        with col2:
            st.write("**Embedding:**", "‚úÖ S·∫µn s√†ng" if st.session_state.embeddings else "‚ùå Ch∆∞a t·∫£i")
            st.write("**RAG:**", "‚úÖ S·∫µn s√†ng" if st.session_state.rag_chain else "‚ùå Ch∆∞a c√≥ t√†i li·ªáu")
    
    # Enhanced chunking info
    with st.expander("üöÄ Enhanced Chunking Features"):
        st.markdown("""
        ### ‚ú® C·∫£i ti·∫øn m·ªõi:
        - **üî• Hybrid Strategy**: Balance t·ªëc ƒë·ªô vs ch·∫•t l∆∞·ª£ng
        - **üíæ Smart Caching**: Cache chunks ƒë·ªÉ load instant
        - **üìä Progress Tracking**: Real-time status updates
        - **üîó Context Overlap**: Preserve context gi·ªØa chunks
        - **‚öôÔ∏è Configurable**: Customize theo needs
        - **üõ°Ô∏è Robust Fallback**: Auto-fallback n·∫øu c√≥ l·ªói
        """)
        
        # Current settings display
        settings = st.session_state.get("chunking_settings", {})
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üìä Strategy", settings.get("strategy", "hybrid"))
        with col2:
            st.metric("üìè Chunk Size", f"{settings.get('chunk_size', 1000)} chars")
        with col3:
            st.metric("üîó Overlap", f"{settings.get('overlap', 100)} chars")
            
    # H∆∞·ªõng d·∫´n chi ti·∫øt
    st.markdown("""
    ### üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:
    1. **‚öôÔ∏è C·∫•u h√¨nh**: Ch·ªçn chunking strategy ·ªü sidebar
    2. **üìÇ T·∫£i t√†i li·ªáu**: Ch·ªçn file PDF ·ªü sidebar b√™n tr√°i
    3. **üöÄ X·ª≠ l√Ω**: Nh·∫•n n√∫t "X·ª≠ l√Ω PDF v·ªõi Enhanced Chunking"
    4. **üí¨ Tr√≤ chuy·ªán**: B·∫Øt ƒë·∫ßu h·ªèi ƒë√°p v·ªÅ n·ªôi dung t√†i li·ªáu
    5. **üíæ Qu·∫£n l√Ω**: L∆∞u, t·∫£i l·∫°i ho·∫∑c x√≥a c√°c cu·ªôc tr√≤ chuy·ªán
    
    ### üéØ Performance Tips:
    - **Hybrid strategy**: T·ªët nh·∫•t cho h·∫ßu h·∫øt documents
    - **Enable cache**: L·∫ßn ƒë·∫ßu ch·∫≠m, l·∫ßn sau instant
    - **Fixed strategy**: D√πng cho documents l·ªõn (>20 pages)
    - **Semantic strategy**: D√πng cho documents quan tr·ªçng c·∫ßn ƒë·ªô ch√≠nh x√°c cao
    """)

# =================================================================
# X·ª¨ L√ù REFRESH GIAO DI·ªÜN
# =================================================================

# Ki·ªÉm tra v√† th·ª±c hi·ªán refresh n·∫øu c·∫ßn
if st.session_state.get('refresh_needed', False):
    st.session_state['refresh_needed'] = False
    st.rerun() 